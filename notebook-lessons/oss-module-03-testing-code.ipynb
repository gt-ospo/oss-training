{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb47eff3a262dce",
   "metadata": {
    "id": "3bb47eff3a262dce"
   },
   "source": [
    "# Testing: What, Why and How To"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a69328ea7acb4e",
   "metadata": {
    "id": "34a69328ea7acb4e"
   },
   "source": [
    "## The Many Types of [Software] Tests\n",
    "\n",
    "There are many types of tests broadly divided into two categories:\n",
    "\n",
    "* **functional testing** ‚¨ú üìú testing the functionality\n",
    "* **non-functional testing** üí£ üî® testing the system as a whole without regard for intended functionality\n",
    "\n",
    "Another dimension you can categorize tests by is whether they are:\n",
    "* **automated** ü§ñ\n",
    "* or **manual** üßë\n",
    "\n",
    "The specific categories of tests you will most likely see out in the wild include:\n",
    "\n",
    "* **unit** testing (atomic functionality)\n",
    "* **integration** testing (interrelated functionality)\n",
    "* **user acceptance** testing (does it fulfill the user contract)\n",
    "* **compliance** testing (does it comply with prescribed regulations)\n",
    "* **end-to-end** testing (full user/system workflows)\n",
    "* **usability** testing (human friendly)\n",
    "* **accessibility** testing (disability friendly)\n",
    "* **load** testing (regular traffic)\n",
    "* **stress** testing (worst case traffic)\n",
    "* **penetration** testing (hackers)\n",
    "* **fuzz** testing (random inputs)\n",
    "* **formatting** testing (does the code conform to defined formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oOsw2ccf8n6t",
   "metadata": {
    "id": "oOsw2ccf8n6t"
   },
   "source": [
    "#### We will be focusing on functional, automated, **unit tests**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nDynVot36kMs",
   "metadata": {
    "id": "nDynVot36kMs"
   },
   "source": [
    "## Why should we take the time to write automated tests?\n",
    "\n",
    "Testing is a fundamental practice in software development. It allows us to:\n",
    "\n",
    "* Tests provide **documentation** üìú of how our code is supposed to work for collaborators and for our future selves\n",
    "\n",
    "* Tests help **engineers onboard into established codebases** without anxiety üò¨ üôà\n",
    "\n",
    "* Regular testing helps maintain the reliability and stability of our codebase so that we can **confidently expand** our projects without time wasted tracking down regressions\n",
    "\n",
    "* Ensure code quality by **catching bugs üêõ early** and keeping them **out of production** üö®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3UG2YSpyV0Yv",
   "metadata": {
    "id": "3UG2YSpyV0Yv"
   },
   "source": [
    " There are **many languages** out there. We have to choose one, so we'll use Python here. The **high level concepts and features of our testing framework** are **fairly universal**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ad7c221d26da8",
   "metadata": {
    "id": "307ad7c221d26da8"
   },
   "source": [
    "## Test-Driven Development\n",
    "\n",
    "Test-Driven Development (TDD) is an iterative software development approach that encourages writing tests before writing the actual code.\n",
    "\n",
    "### Fail Fast üí£ üöÄ\n",
    "\n",
    "Writing the test first makes you stop to **carefully consider** what a function really needs to do. You're writing a specification, otherwise referred to as a requirements document, before you write your \"real\" code.\n",
    "\n",
    "You will sometimes see the TDD cycle referred to as:\n",
    "\n",
    "### üî¥üü¢üîÑ Red Green Refactor\n",
    "\n",
    "1. üî¥ **Write** a test that defines the expected behavior and outcomes of the function\n",
    "2. üî¥ **Run** the test and see that it fails because there's no functionality defined yet\n",
    "3. üü¢ **Write** the minimum code to pass the test\n",
    "4. üü¢ **Run** the test... see that it succeeds (if it fails... keep at it!)\n",
    "5. üîÑ **Check** over your function and your test, make any **tweaks** to make the code better or more descriptive/self-documenting/faster\n",
    "\n",
    "**Cycle** between tweaking the function and running the test until the test passes and you are happy with the functionality and the test code.\n",
    "\n",
    "Working in this way forces you to be **intentional** about the code you write and to really think through what the inputs and outputs should be. It also prevents the inevitable headache of debugging an **\"over-engineered\" monolith** of code... which is extremely demoralizing.\n",
    "\n",
    "The resulting code will be more **modular** and decoupled, making it more **reusable**. You'll find your code being used by other engineers because your test-covered, self-documenting functions are consistently reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5m9YOkdepjra",
   "metadata": {
    "id": "5m9YOkdepjra"
   },
   "source": [
    "## The pytest Testing Framework\n",
    "\n",
    "`unittest` is baked in to Python, but we'll be using [pytest](https://realpython.com/pytest-python-testing/)  \n",
    "- less setup\n",
    "- simple syntax\n",
    "- large ecosystem of [plugins](https://pytest.org/en/stable/reference/plugin_list.html)\n",
    "\n",
    "- auto-finds tests, just as long as you make sure your test files and functions start with `test_`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6908ba43b63307",
   "metadata": {
    "id": "1a6908ba43b63307"
   },
   "source": [
    "### First Unit Test\n",
    "\n",
    "We're going to write some utility functions for working with grades. First lets set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b253ff18d75a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T19:03:00.526096Z",
     "start_time": "2024-06-02T19:03:00.522455Z"
    },
    "id": "cc5b253ff18d75a3"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# set up code directory\n",
    "mkdir -p gradebook\n",
    "\n",
    "# colab will complain later without this\n",
    "#apt install python3.10-venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gDfhxjerYIOg",
   "metadata": {
    "id": "gDfhxjerYIOg"
   },
   "outputs": [],
   "source": [
    "# set up python\n",
    "!python3 -m venv venv\n",
    "!source venv/bin/activate\n",
    "!python3 -m pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89e32a890476cc",
   "metadata": {
    "id": "df89e32a890476cc"
   },
   "source": [
    "We have to remember a few things as we start to write unit tests.\n",
    "\n",
    "* Unit tests are designed to **test individual bits of logic** (\"units\"), self-contained, **no dependencies**\n",
    "* Each test should focus on a single behavior or outcome, i.e. **don't try to cover every possible scenario or outcome in one test**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mLtDFSbJLUgG",
   "metadata": {
    "id": "mLtDFSbJLUgG"
   },
   "source": [
    "You should approach building unit tests keeping in mind 3 steps:\n",
    "\n",
    "\n",
    "#### 1. Arrange\n",
    "set up the conditions for your test\n",
    "\n",
    "#### 2. Act\n",
    "invoke the unit of code being tested\n",
    "\n",
    "#### 3. Assert\n",
    "check that the result is what was expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rAp5E3DBPsT-",
   "metadata": {
    "id": "rAp5E3DBPsT-"
   },
   "outputs": [],
   "source": [
    "!mkdir test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83dafe379c9cde2",
   "metadata": {
    "id": "a83dafe379c9cde2"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_averaging.py\n",
    "import pytest\n",
    "from gradebook.grade_utils import calculate_average\n",
    "\n",
    "def test_averaging():\n",
    "    grades = [90, 80, 70] # arrange\n",
    "    average = calculate_average(grades) # act\n",
    "    assert average == 80 # assert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e9d4767a25046",
   "metadata": {
    "id": "7e1e9d4767a25046"
   },
   "source": [
    "Now lets run `pytest`. Remember that we're **anticipating failure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115358b3cfe43f8b",
   "metadata": {
    "id": "115358b3cfe43f8b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22OS2PnOb4TJ",
   "metadata": {
    "id": "22OS2PnOb4TJ"
   },
   "source": [
    "**That looks about right.**\n",
    "\n",
    "Lets add a basic implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xfKV8QpEWgeS",
   "metadata": {
    "id": "xfKV8QpEWgeS"
   },
   "outputs": [],
   "source": [
    "!touch gradebook/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2MDKa-HbI6ov",
   "metadata": {
    "id": "2MDKa-HbI6ov"
   },
   "outputs": [],
   "source": [
    "%%writefile gradebook/grade_utils.py\n",
    "\n",
    "def calculate_average(grades):\n",
    "    return sum(grades) / len(grades)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AWfaS7-TJzK8",
   "metadata": {
    "id": "AWfaS7-TJzK8"
   },
   "source": [
    "Re-run our test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfKIMTBJJ4BL",
   "metadata": {
    "id": "cfKIMTBJJ4BL"
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3B6VN3yXKAbx",
   "metadata": {
    "id": "3B6VN3yXKAbx"
   },
   "source": [
    "I do üíö love üíö seeing that <span style='color:green'>green</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5gHmSlVybr-n",
   "metadata": {
    "id": "5gHmSlVybr-n"
   },
   "source": [
    "If you want to see which specific tests were executed, you can pass the `--verbose` or `-v` parameter to pytest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lMKVtfynb1A-",
   "metadata": {
    "id": "lMKVtfynb1A-"
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83708efa54d164a4",
   "metadata": {
    "id": "83708efa54d164a4"
   },
   "source": [
    "Test names should always be **descriptive**, so lets give it a better name for those who come after us, and trim it down to be more Python-y now that you get the **arrange-act-assert** point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb24ac5223439c",
   "metadata": {
    "id": "61bb24ac5223439c"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_averaging.py\n",
    "import pytest\n",
    "from gradebook.grade_utils import calculate_average\n",
    "\n",
    "def test_that_average_grade_returns_average_of_grades_provided():\n",
    "    assert calculate_average([90, 80, 70]) == 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JjtBhtqTLZlR",
   "metadata": {
    "id": "JjtBhtqTLZlR"
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9afcabc30f653",
   "metadata": {
    "id": "1cd9afcabc30f653"
   },
   "source": [
    "## Fixtures\n",
    "\n",
    "Technically, we should not be hardcoding any values. We should use fixtures instead.\n",
    "\n",
    "This helps by **documenting** the expected values for anyone who looks at this code and also facilitates their **adding more tests** by giving them building blocks to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca53376ce1b04b02",
   "metadata": {
    "id": "ca53376ce1b04b02"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_averaging.py\n",
    "import pytest\n",
    "from gradebook.grade_utils import calculate_average\n",
    "\n",
    "@pytest.fixture\n",
    "def some_grades():\n",
    "    return [90, 80, 70]\n",
    "\n",
    "def test_that_average_grade_returns_average_of_grades_provided(some_grades):\n",
    "    assert calculate_average(some_grades) == 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MptAHDz0QUUL",
   "metadata": {
    "id": "MptAHDz0QUUL"
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DWN0uNw8mnZa",
   "metadata": {
    "id": "DWN0uNw8mnZa"
   },
   "source": [
    "**Maybe** we should discuss that `pytest` **abbreviated output**.\n",
    "\n",
    "- A green dot `.` means that the test passed\n",
    "- An `F` means that the test has failed\n",
    "- An `E` means that the test raised an unexpected exception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d9c6055c7b6d6",
   "metadata": {
    "id": "322d9c6055c7b6d6"
   },
   "source": [
    "Lets step back and consider whether this test covers every situation we may want to test for. ü§î\n",
    "\n",
    "What if the list is **empty**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7iLsIgb5V4U_",
   "metadata": {
    "id": "7iLsIgb5V4U_"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_averaging_some_more.py\n",
    "import pytest\n",
    "from gradebook.grade_utils import calculate_average\n",
    "\n",
    "@pytest.fixture\n",
    "def no_grades():\n",
    "    return []\n",
    "\n",
    "def test_that_average_of_no_grades_still_works(no_grades):\n",
    "    assert calculate_average(no_grades) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1wSzOwW5Yu_9",
   "metadata": {
    "id": "1wSzOwW5Yu_9"
   },
   "outputs": [],
   "source": [
    "!python -m pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ek_rQDo5Y1md",
   "metadata": {
    "id": "Ek_rQDo5Y1md"
   },
   "source": [
    "**Okay, now, lets **refactor** our function to handle the empty list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DGpd1EigY08v",
   "metadata": {
    "id": "DGpd1EigY08v"
   },
   "outputs": [],
   "source": [
    "%%writefile gradebook/grade_utils.py\n",
    "\n",
    "def calculate_average(grades):\n",
    "    if not grades:\n",
    "      return None\n",
    "    return sum(grades) / len(grades)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18eacfc7aa09188",
   "metadata": {
    "id": "d18eacfc7aa09188"
   },
   "source": [
    "I maintain that the average of an empty list is nothing, sending a clear message that the list of grades was empty and not a set of zeros, which could be a valid input.\n",
    "\n",
    "Gathering requirements is a topic for another day, but just wanted to point out how **writing tests first helps to shed light on ambiguity** so you can have a chat with the user requesting the change or the \"product owner\" in a larger org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34xqDVtVWFxa",
   "metadata": {
    "id": "34xqDVtVWFxa"
   },
   "outputs": [],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0qF5gjQ2NkPR",
   "metadata": {
    "id": "0qF5gjQ2NkPR"
   },
   "source": [
    "And, you know, erring on the side of caution isn't a bad thing. It doesn't hurt to introduce tests just to make sure things work how you expect, e.g. if the input is not an empty list but nothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t3q7yjyIbsLv",
   "metadata": {
    "id": "t3q7yjyIbsLv"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_averaging_yet_again.py\n",
    "import pytest\n",
    "from gradebook.grade_utils import calculate_average\n",
    "\n",
    "def test_that_average_of_nothing_still_does_what_is_expected():\n",
    "    assert calculate_average(None) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PbJ6_u3-cEvu",
   "metadata": {
    "id": "PbJ6_u3-cEvu"
   },
   "outputs": [],
   "source": [
    "!python -m pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732189433a1cdfb",
   "metadata": {
    "id": "1732189433a1cdfb"
   },
   "source": [
    "## Markers\n",
    "\n",
    "In a **large codebase**, I have seen **automated tests take upwards of 30 minutes** to run because there are **so many**, each atomic, each \"arranging\" its setup. It may be beneficial to start using [markers](https://pytest-with-eric.com/pytest-best-practices/pytest-markers/) straight out of the gate so you can run only the tests in the scope you are modifying.\n",
    "\n",
    "This will also come in handy if you are deploying to multiple platforms and your test is only relevant to **specific architectures** or **specific versions** of dependencies. You can then configure CI/CD to run only a subset of tests depending which version you are building.\n",
    "\n",
    "**Markers** allow you to group your unit tests and run them by marker.\n",
    "\n",
    "Lets apply markers to 2 of the 3 tests and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GHfkTIR4g6kS",
   "metadata": {
    "id": "GHfkTIR4g6kS"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_averaging_yet_again.py\n",
    "import pytest\n",
    "from gradebook.grade_utils import calculate_average\n",
    "\n",
    "@pytest.mark.average\n",
    "def test_that_average_of_nothing_still_does_what_is_expected():\n",
    "    assert calculate_average(None) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csHvQhCrhSaY",
   "metadata": {
    "id": "csHvQhCrhSaY"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_averaging_some_more.py\n",
    "import pytest\n",
    "from gradebook.grade_utils import calculate_average\n",
    "\n",
    "@pytest.fixture\n",
    "def no_grades():\n",
    "    return []\n",
    "\n",
    "@pytest.mark.average\n",
    "def test_that_average_of_no_grades_still_works(no_grades):\n",
    "    assert calculate_average(no_grades) is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ky6rNSIRhpni",
   "metadata": {
    "id": "Ky6rNSIRhpni"
   },
   "source": [
    "All three of our tests will still run with no parameters passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hHLhe9ePhh0m",
   "metadata": {
    "id": "hHLhe9ePhh0m"
   },
   "outputs": [],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ifUfaGChulV",
   "metadata": {
    "id": "5ifUfaGChulV"
   },
   "source": [
    "But if we pass the marker parameter with the marker name \"average\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JuKf8W4Ph8aP",
   "metadata": {
    "id": "JuKf8W4Ph8aP"
   },
   "outputs": [],
   "source": [
    "!python -m pytest -m average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DvfgeRD-imAG",
   "metadata": {
    "id": "DvfgeRD-imAG"
   },
   "source": [
    "It's just a warning, but we should really register our markers, if only to keep a record of what markers are available when other engineers run our tests and want to find out what markers there are to choose from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xk3WUgi8iwpN",
   "metadata": {
    "id": "xk3WUgi8iwpN"
   },
   "outputs": [],
   "source": [
    "!python -m pytest --markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9jr2vWc0jN6t",
   "metadata": {
    "id": "9jr2vWc0jN6t"
   },
   "source": [
    "We can add our custom marker to a dedicated `pytest.ini` file or a `pyproject.toml` file. I would recommend adding to `pyproject.toml` if there is one. It's always good to go best practice instead of ignoring these warnings so let's configure a `pyproject.toml` so we stop seeing this warning in subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nAgIj_BGjqPb",
   "metadata": {
    "id": "nAgIj_BGjqPb"
   },
   "outputs": [],
   "source": [
    "%%writefile pyproject.toml\n",
    "[tool.pytest.ini_options]\n",
    "markers = [\n",
    "    \"average: used to mark all tests associated with averaging grades\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KFmH6mdYkFvW",
   "metadata": {
    "id": "KFmH6mdYkFvW"
   },
   "outputs": [],
   "source": [
    "!python -m pytest --markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yTyJhxmykHUJ",
   "metadata": {
    "id": "yTyJhxmykHUJ"
   },
   "source": [
    "**Neat!**\n",
    "\n",
    "There are many other configuration settings for `pytest`. Read more about them in the [official docs](https://docs.pytest.org/en/7.1.x/reference/reference.html#ini-options-ref).\n",
    "\n",
    "Now lets see if the warning went away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6zYyDAkNtC",
   "metadata": {
    "id": "8a6zYyDAkNtC"
   },
   "outputs": [],
   "source": [
    "!python -m pytest -m average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1y6AAp6RkTjQ",
   "metadata": {
    "id": "1y6AAp6RkTjQ"
   },
   "source": [
    "Love it! You can also **exclude** a marker instead of including it by quoting the marker and preceding with a `not`, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1thHqA-TkaSv",
   "metadata": {
    "id": "1thHqA-TkaSv"
   },
   "outputs": [],
   "source": [
    "!python -m pytest -m \"not average\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k-fANXc_ksZw",
   "metadata": {
    "id": "k-fANXc_ksZw"
   },
   "source": [
    "Lets go back and add that `average` marker to the 3rd test to be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mbBt6toQhabK",
   "metadata": {
    "id": "mbBt6toQhabK"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_averaging.py\n",
    "import pytest\n",
    "from gradebook.grade_utils import calculate_average\n",
    "\n",
    "@pytest.fixture\n",
    "def some_grades():\n",
    "    return [90, 80, 70]\n",
    "\n",
    "@pytest.mark.average\n",
    "def test_that_average_grade_returns_average_of_grades_provided(some_grades):\n",
    "    assert calculate_average(some_grades) == 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-FNFF7I4gw5y",
   "metadata": {
    "id": "-FNFF7I4gw5y"
   },
   "source": [
    "Test that it worked. We should have no tests that are not marked average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rKSxWJD3kzs8",
   "metadata": {
    "id": "rKSxWJD3kzs8"
   },
   "outputs": [],
   "source": [
    "!python -m pytest -m \"not average\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gKIle7ANhcKS",
   "metadata": {
    "id": "gKIle7ANhcKS"
   },
   "source": [
    "Lets add a few more tests so we can see how fixtures can be shared. Here I will borrow the `some_grades` fixture from earlier in our new tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e01c2f527e81ec",
   "metadata": {
    "id": "a9e01c2f527e81ec"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_highest_lowest.py\n",
    "import pytest\n",
    "from gradebook.grade_utils import find_highest_grade, find_lowest_grade\n",
    "\n",
    "@pytest.mark.hilo\n",
    "@pytest.mark.highest\n",
    "def test_find_highest_grade(some_grades):\n",
    "    assert find_highest_grade(some_grades) == 90\n",
    "\n",
    "@pytest.mark.hilo\n",
    "@pytest.mark.lowest\n",
    "def test_find_lowest_grade(some_grades):\n",
    "    assert find_lowest_grade(some_grades) == 70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "puLCTY4Yn-In",
   "metadata": {
    "id": "puLCTY4Yn-In"
   },
   "source": [
    "Notice that we are applying **multiple markers** to our new tests.\n",
    "\n",
    "I will go ahead and create the logic as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7vb1Nn2An9TS",
   "metadata": {
    "id": "7vb1Nn2An9TS"
   },
   "outputs": [],
   "source": [
    "%%writefile gradebook/grade_utils.py\n",
    "\n",
    "def calculate_average(grades):\n",
    "    \"\"\"Return the average grade\"\"\"\n",
    "    if not grades:\n",
    "      return None\n",
    "    return sum(grades) / len(grades)\n",
    "\n",
    "\n",
    "def find_highest_grade(grades):\n",
    "    \"\"\"Return the highest grade\"\"\"\n",
    "    if not grades:\n",
    "        return None\n",
    "    return max(grades)\n",
    "\n",
    "\n",
    "def find_lowest_grade(grades):\n",
    "    \"\"\"Return the lowest grade\"\"\"\n",
    "    if not grades:\n",
    "        return None\n",
    "    return min(grades)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197bd456c71d3a29",
   "metadata": {
    "id": "197bd456c71d3a29"
   },
   "outputs": [],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hpgm9F5vopps",
   "metadata": {
    "id": "hpgm9F5vopps"
   },
   "source": [
    "Okay, so we have a few **new markers** that we will register, but the only **error** is that our fixture is `not found`. Lets address that fixture first. We can share fixtures with all of our tests. Lets move `some_grades` to a `conftest.py` (`pytest` does expect the shared fixtures to live in a file of this specific name) so that we can have a place to add fixtures as our codebase grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IfAsffoYqJ7M",
   "metadata": {
    "id": "IfAsffoYqJ7M"
   },
   "outputs": [],
   "source": [
    "%%writefile test/conftest.py\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope=\"function\")\n",
    "def some_grades():\n",
    "    return [90, 80, 70]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011cH189jcC6",
   "metadata": {
    "id": "011cH189jcC6"
   },
   "source": [
    "When fixtures are defined in `conftest.py`, they can have a **scope** applied. The scope determines when the fixture will be invoked. In this case, this fixture will be invoked for each function in which it is used. This is actually the **default**.\n",
    "\n",
    "There are several options because some fixtures are complex. You can instantiate a class or a service as a fixture, and you will not want **expensive operations** to be performed more often than necessary.\n",
    "\n",
    "The other options are:\n",
    "\n",
    "- class\n",
    "- module\n",
    "- session\n",
    "\n",
    "[Real world example](https://github.com/getsentry/sentry/blob/a471c8ad53dcd956f6b886c06ee1555697966002/src/sentry/testutils/pytest/kafka.py#L14.) of why you might want to scope fixtures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WdNAE3bCRbFb",
   "metadata": {
    "id": "WdNAE3bCRbFb"
   },
   "source": [
    "Lets **remove** the fixture from where it had originally been declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OaCPrO9YrbDA",
   "metadata": {
    "id": "OaCPrO9YrbDA"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_averaging.py\n",
    "import pytest\n",
    "from gradebook.grade_utils import calculate_average\n",
    "\n",
    "@pytest.mark.average\n",
    "def test_that_average_grade_returns_average_of_grades_provided(some_grades):\n",
    "    assert calculate_average(some_grades) == 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QZeUuidvqioE",
   "metadata": {
    "id": "QZeUuidvqioE"
   },
   "source": [
    "Lets also **register our new markers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BfbWwXTHojZ7",
   "metadata": {
    "id": "BfbWwXTHojZ7"
   },
   "outputs": [],
   "source": [
    "%%writefile pyproject.toml\n",
    "[tool.pytest.ini_options]\n",
    "markers = [\n",
    "    \"average: used to mark all tests associated with averaging grades\",\n",
    "    \"hilo: used to mark tests associated with identifying the extreme grades\",\n",
    "    \"highest: used to mark tests associated with identfying the highest grade\",\n",
    "    \"lowest: used to mark tests associated with identifying the lowest grade\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5lIlE3M6pViO",
   "metadata": {
    "id": "5lIlE3M6pViO"
   },
   "source": [
    "After applying **multiple markers** to the same tests, we can now refer to any of the markers associated with a test to run that test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A8XH-Jxophgo",
   "metadata": {
    "id": "A8XH-Jxophgo"
   },
   "outputs": [],
   "source": [
    "!python -m pytest -m hilo -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BhP2gnlbujyw",
   "metadata": {
    "id": "BhP2gnlbujyw"
   },
   "source": [
    "Both of the tests in the hilo file ran!\n",
    "\n",
    "And to demonstrate the point that we can use **any of the markers** applied to a test to run that test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ALmf-GOXuqjz",
   "metadata": {
    "id": "ALmf-GOXuqjz"
   },
   "outputs": [],
   "source": [
    "!python -m pytest -m highest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f561b41c6079f2c2",
   "metadata": {
    "id": "f561b41c6079f2c2"
   },
   "source": [
    "Lots of marker settings available, including **conditional skipping** and **intentional failure**, which can be used as a placeholder to address an issue, and **timeout** markers that will cause the test to error if it exceeds the set time.\n",
    "\n",
    "More examples with explanations can be found on this handy blog: https://pytest-with-eric.com/pytest-best-practices/pytest-markers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D-inP5afas8E",
   "metadata": {
    "id": "D-inP5afas8E"
   },
   "source": [
    "Markers can also be used to [skip tests based on conditions](https://github.com/urllib3/urllib3/blob/da410581b6b3df73da976b5ce5eb20a4bd030437/dummyserver/testcase.py#L314)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6a81fd549ce57",
   "metadata": {
    "id": "2cf6a81fd549ce57"
   },
   "source": [
    "Before we add more tests, some additional considerations...\n",
    "\n",
    "* Tests should not be **interdependent**.\n",
    "* Tests need to be able to run **in any order**.\n",
    "* Unit tests should have a low **execution time** because there will be many of them.\n",
    "* Tests need to be run **frequently**, preferably **automatically** as a pre-commit hook or, much better, as part of a CI workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3-qlT1OtgQHp",
   "metadata": {
    "id": "3-qlT1OtgQHp"
   },
   "outputs": [],
   "source": [
    "!python -m pytest --durations=0 -vv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd744bc7ee9f0c82",
   "metadata": {
    "id": "cd744bc7ee9f0c82"
   },
   "source": [
    "Lets add some more tests and some more functions and then rerun our new test suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FXOTWFftf0dD",
   "metadata": {
    "id": "FXOTWFftf0dD"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_integrated_functionality.py\n",
    "import pytest\n",
    "from gradebook.grade_utils import calculate_average, determine_letter_grade\n",
    "\n",
    "def test_letter_grade_average(some_grades):\n",
    "    # calculate_average\n",
    "    average = calculate_average(some_grades)\n",
    "\n",
    "    # determine_letter_grade for the average\n",
    "    average_letter_grade = determine_letter_grade(average)\n",
    "    assert average_letter_grade == \"B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166debb8921957d4",
   "metadata": {
    "id": "166debb8921957d4"
   },
   "source": [
    "**That's technically a super lightweight **integration** test because it checks that two different parts of our codebase work together.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EOwQ4Q_xmhph",
   "metadata": {
    "id": "EOwQ4Q_xmhph"
   },
   "source": [
    "Let's add the code to return a letter grade for a numeric grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N4BYm2tGlfai",
   "metadata": {
    "id": "N4BYm2tGlfai"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << _EOF >> gradebook/grade_utils.py\n",
    "\n",
    "\n",
    "def determine_letter_grade(grade):\n",
    "    \"\"\"Return the letter grade for a numeric grade.\"\"\"\n",
    "    if grade >= 90:\n",
    "        return \"A\"\n",
    "    elif grade >= 80:\n",
    "        return \"B\"\n",
    "    elif grade >= 70:\n",
    "        return \"C\"\n",
    "    elif grade >= 60:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"F\"\n",
    "_EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab32cfdedf2b9e3",
   "metadata": {
    "id": "6ab32cfdedf2b9e3"
   },
   "outputs": [],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3uWXN2RHvVcH",
   "metadata": {
    "id": "3uWXN2RHvVcH"
   },
   "source": [
    "## Parametrization\n",
    "\n",
    "Now that we've added the letter grade function, it would be a great time to bring up **parametrization**. Parametrization comes in handy if you find yourself writing a lot of tests that look very similar, or if you have to test for a long list of scenarios that could be easily expressed as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MmqFAAgPv2WU",
   "metadata": {
    "id": "MmqFAAgPv2WU"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_parametrized_letter_grades.py\n",
    "import pytest\n",
    "\n",
    "from gradebook.grade_utils import determine_letter_grade\n",
    "\n",
    "\n",
    "grade_ranges = {\n",
    "    \"A\": range(90, 101),\n",
    "    \"B\": range(80, 90),\n",
    "    \"C\": range(70, 80),\n",
    "    \"D\": range(60, 70),\n",
    "    \"F\": range(0, 60),\n",
    "}\n",
    "\n",
    "@pytest.mark.parametrize(\"letter,number\",\n",
    "                         [(letter, number) for letter, numbers in grade_ranges.items() for number in numbers])\n",
    "\n",
    "def test_is_letter_grade(letter, number):\n",
    "    assert determine_letter_grade(number) == letter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nDuMQSoHv_WZ",
   "metadata": {
    "id": "nDuMQSoHv_WZ"
   },
   "source": [
    "***Brace yourself...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ALitQF1wDdn",
   "metadata": {
    "id": "8ALitQF1wDdn"
   },
   "outputs": [],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5Kqtt5W2kHlG",
   "metadata": {
    "id": "5Kqtt5W2kHlG"
   },
   "outputs": [],
   "source": [
    "!python -m pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RNqdmG9fSm8Z",
   "metadata": {
    "id": "RNqdmG9fSm8Z"
   },
   "source": [
    "Now that we've looked at a silly example, a [real example of parametrization](https://github.com/tiangolo/fastapi/blob/a9819dfd8da39a754837cc134df4aca6c0a9a3f6/tests/test_param_include_in_schema.py#L168)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee3243dd5bd362",
   "metadata": {
    "id": "aaee3243dd5bd362"
   },
   "source": [
    "## Mocking Complex Dependencies\n",
    "\n",
    "The last feature that will be essential as you begin to work with established codebases is **mocking**. [`pytest-mock`](https://pytest-mock.readthedocs.io/en/latest/usage.html) is a `pytest` plugin that provides mocking functionality. You should not connect to an actual database or your 3rd party API during unit testing. You should \"mock\" these dependencies instead.\n",
    "\n",
    "### Why mock?\n",
    "Because unit tests should be isolated, have **no dependencies**, and be **fast**, avoiding latency like that found in networking and database or disk operations\n",
    "\n",
    "You can mock pretty much anything:\n",
    "- HTTP requests\n",
    "- Database query results\n",
    "- File system manipulation\n",
    "- Built-in functions and constants\n",
    "- Complex classes we do not want to have to initialize\n",
    "- 3rd party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e54134c294d53e1",
   "metadata": {
    "id": "8e54134c294d53e1"
   },
   "outputs": [],
   "source": [
    "!python -m pip install pytest-mock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HGxmgW7-rtyg",
   "metadata": {
    "id": "HGxmgW7-rtyg"
   },
   "source": [
    "Now we will create a simple function to write the grades list to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U-TQSe_pragx",
   "metadata": {
    "id": "U-TQSe_pragx"
   },
   "outputs": [],
   "source": [
    "%%writefile gradebook/save_grades.py\n",
    "def write_to_file(grades) -> None:\n",
    "    \"\"\"\n",
    "    Function to write our grades to a file\n",
    "    :param grades: grades list\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with open(f\"grades.txt\", \"w\") as f:\n",
    "        f.write(str(grades))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eiGlZi1gr8uh",
   "metadata": {
    "id": "eiGlZi1gr8uh"
   },
   "source": [
    "This function opens a file and writes the grades. Super simple. We don't want to actually open a file or actually write to a file, so we will mock the opening and writing. We will intercept these calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kcn1Mu5As4Vj",
   "metadata": {
    "id": "kcn1Mu5As4Vj"
   },
   "outputs": [],
   "source": [
    "%%writefile test/test_write_to_file.py\n",
    "import pytest\n",
    "\n",
    "from gradebook.save_grades import write_to_file\n",
    "\n",
    "# first we pass the mocker in\n",
    "def test_write_grades_to_file(mocker):\n",
    "    \"\"\"\n",
    "    Function to test writing grades to a file\n",
    "    \"\"\"\n",
    "\n",
    "    # mock the 'open' function call to return a file object\n",
    "    # using a builtin from unittest\n",
    "    mock_file = mocker.mock_open()\n",
    "    mocker.patch(\"builtins.open\", mock_file)\n",
    "\n",
    "    # now we can call our function that writes to a file\n",
    "    write_to_file([50,75,100])\n",
    "\n",
    "    # assert that the 'open' function was called with the expected arguments\n",
    "    mock_file.assert_called_once_with(\"grades.txt\", \"w\")\n",
    "\n",
    "    # assert that the file was written to with the expected text\n",
    "    mock_file().write.assert_called_once_with(str([50,75,100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q9aRd_C5uVD-",
   "metadata": {
    "id": "q9aRd_C5uVD-"
   },
   "outputs": [],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OHKbt4oP0E3_",
   "metadata": {
    "id": "OHKbt4oP0E3_"
   },
   "source": [
    "Beyond mocks there are also **spies**, but mocks are plenty on their own. Read all about mocking features in the [docs](https://pytest-mock.readthedocs.io/en/latest/usage.html). Mocks replace functionality with hardcoded values and spies replace only portions of real classes/modules which can be useful when you want to make sure that a deeper method in a 3rd party library or legacy codebase was invoked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UFMGfqgXaLY7",
   "metadata": {
    "id": "UFMGfqgXaLY7"
   },
   "source": [
    "#### Here's [a real mock example](https://github.com/microsoft/timewarp/blob/44dca8474cb6182458830677763261cffccfaac4/utilities/fixtures.py#L80) from Microsoft!\n",
    "\n",
    "#### And a [real spy example](https://github.com/slackapi/python-slack-events-api/blob/2884d7d21fea634d1e5e7926409ed87f6fcc14cf/tests/test_server.py#L179) from Slack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a2d58faccc1",
   "metadata": {
    "id": "519a2d58faccc1"
   },
   "source": [
    "## Code Coverage\n",
    "\n",
    "**Aim** for **high** code coverage, but don't obsess.\n",
    "\n",
    "The coverage **Statement** coverage measures how many statements in the code were executed\n",
    "\n",
    "Lets see what our coverage looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a00635d9e81f0",
   "metadata": {
    "id": "a8a00635d9e81f0"
   },
   "outputs": [],
   "source": [
    "!python -m pip install coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bXtp5LV4ChW",
   "metadata": {
    "id": "2bXtp5LV4ChW"
   },
   "outputs": [],
   "source": [
    "!python -m coverage run -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1pXg6sM4SLj",
   "metadata": {
    "id": "n1pXg6sM4SLj"
   },
   "source": [
    "Well that looks exactly like our usual `pytest` output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LH1y3Edv4Vqy",
   "metadata": {
    "id": "LH1y3Edv4Vqy"
   },
   "outputs": [],
   "source": [
    "!python -m coverage report -m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4df4ab7bcdbc9",
   "metadata": {
    "id": "b4c4df4ab7bcdbc9"
   },
   "source": [
    "***That's pretty good!***\n",
    "\n",
    "In case you're wondering what those headings mean:  \n",
    "\n",
    "**Stmts** The total number of statements in the package.  \n",
    "**Miss** The number of statements that were not executed during testing.  \n",
    "**Cover** The percentage of statements that were executed during testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uSic6lDU4wA1",
   "metadata": {
    "id": "uSic6lDU4wA1"
   },
   "source": [
    "But say we don't want to have to go poking around in the files trying to figure out where we missed some opportunities to write tests. Say we rather point and click."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EnMzPVmk5Efq",
   "metadata": {
    "id": "EnMzPVmk5Efq"
   },
   "outputs": [],
   "source": [
    "!python -m coverage html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6-eqFjwL6pk8",
   "metadata": {
    "id": "6-eqFjwL6pk8"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "file_path = 'htmlcov/index.html'\n",
    "with open(file_path, 'r') as file:\n",
    "    html_content = file.read()\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SvABrRbuWKar",
   "metadata": {
    "id": "SvABrRbuWKar"
   },
   "source": [
    "**A real example from [FastAPI](https://github.com/tiangolo/fastapi), just check on their impressive 100% code coverage badge to see their report!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af4c999-85b0-45f6-9fc0-14136dcedf11",
   "metadata": {},
   "source": [
    "## Formatting / Linting\n",
    "\n",
    "This might seem aesthetic, but with large, collaborative projects, maintaining conformance to predefined code formatting rules can make a big difference in **readability** and **maintainability**.\n",
    "\n",
    "Consider two ways that the same Python function definition can be written:\n",
    "\n",
    "```python\n",
    "    def my_command(arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8, arg9, arg10)\n",
    "```\n",
    "\n",
    "```python\n",
    "    def my_command(\n",
    "        arg1, \n",
    "        arg2, \n",
    "        arg3, \n",
    "        arg4, \n",
    "        arg5, \n",
    "        arg6, \n",
    "        arg7, \n",
    "        arg8, \n",
    "        arg9, \n",
    "        arg10)\n",
    "```\n",
    "\n",
    "While functionally equivalent, they are very different in style and mixing the two formats in one codebase would decrease readability.\n",
    "\n",
    "There are many tools for linting/formatting, such as:\n",
    "- **Python**: flake8, Ruff\n",
    "- **C++**: clang-format, cpplint\n",
    "- **Javascript**: eslint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14fe6f4-6f0e-4ad0-8591-edf0deafc1c1",
   "metadata": {},
   "source": [
    "Let's give ruff a try on our existing Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbbb65-8a37-40a3-98f4-a27623caf7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m pip install ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29277a42-c267-4589-8569-c7c95abbb49d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m ruff check test/ gradebook/ solutions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877aa82-b984-4a45-a55e-a522b9a7e1dc",
   "metadata": {},
   "source": [
    "Ruff reports a few code quality problems, and tells us that it can automatically fix them. You're welcome to do that, but let's also see what Ruff thinks of our code formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b375b63-8340-43e9-b711-5686b4712878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m ruff format --check test/ gradebook/ solutions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d92b8-1b69-4588-9fae-8914bde56e9e",
   "metadata": {},
   "source": [
    "And Ruff would be happy to reformat the code for us if we removed the `--check` flag."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
